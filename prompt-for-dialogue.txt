{
  "input_data": {
    "topic": "chapter 2.5 Norms from the book of deep learning",
    "topic_context": "2.5 Norms
Sometimes we need to measure the size of a vector. In machine learning, we usually
measure the size of vectors using a function called a norm. Formally, the L
p
 norm is given
by
for p ∈ ℝ, p ≥ 1.
Norms, including the L
p
 norm, are functions mapping vectors to non-negative values.
On an intuitive level, the norm of a vector x measures the distance from the origin to the
point x. More rigorously, a norm is any function f that satisfies the following properties:
f(x) = 0 ⇒ x = 0
f(x + y) ≤ f(x) + f(y) (the triangle inequality)
∀α ∈ ℝ, f(αx) = |α|f(x)
e L
2 norm, with p = 2, is known as the Euclidean norm, which is simply the
Euclidean distance from the origin to the point identified by x. e L
2 norm is used so
frequently in machine learning that it is often denoted simply as ǁxǁ, with the subscript 2
omitted. It is also common to measure the size of a vector using the squared L
2 norm,
which can be calculated simply as x⊤x.
e squared L
2 norm is more convenient to work with mathematically and
computationally than the L
2 norm itself. For example, each derivative of the squared L
2
norm with respect to each element of x depends only on the corresponding element of x,
while all the derivatives of the L
2 norm depend on the entire vector. In many contexts,
the squared L
2 norm may be undesirable because it increases very slowly near the origin.
In several machine learning applications, it is important to discriminate between elements
that are exactly zero and elements that are small but nonzero. In these cases, we turn to a
function that grows at the same rate in all locations, but that retains mathematical
simplicity: the L
1 norm. e L
1 norm may be simplified to
e L
1 norm is commonly used in machine learning when the difference between zero
and nonzero elements is very important. Every time an element of x moves away from 0
by ϵ, the L
1 norm increases by ϵ.
We sometimes measure the size of the vector by counting its number of nonzero
elements. Some authors refer to this function as the “L
0 norm,” but this is incorrect
terminology. e number of nonzero entries in a vector is not a norm, because scaling
the vector by α does not change the number of nonzero entries. e L
1 norm is often
used as a substitute for the number of nonzero entries.
One other norm that commonly arises in machine learning is the L∞ norm, also
known as the max norm. is norm simplifies to the absolute value of the element with
the largest magnitude in the vector,
Sometimes we may also wish to measure the size of a matrix. In the context of deep
learning, the most common way to do this is with the otherwise obscure Frobenius
norm:
which is analogous to the L
2 norm of a vector.
e dot product of two vectors can be rewritten in terms of norms. Specifically,
where θ is the angle between x and y.
"
  },
  "instructions": {
    "Task": "Act as the Executive Producer & Audio-Visual Director for 'Zero Sum'. Your goal is to script a viral educational dialogue optimized for ElevenLabs V3 based on the 'input_data'.",
    "Goal": "Produce a script with a runtime of 10 to 15 minutes. This requires a high word count (approx. 2000-2500 words). The discussion must be exhaustive, covering nuances, counter-arguments, and deep analysis.",
    "Persona": {
      "Character_A": {
        "Role": "The Analyst (Academic)",
        "Traits": "Presents theory exactly as written in textbooks. Logical, precise. Uses tags: [clears throat], [whispers], [deep breath], [fast]."
      },
      "Character_B": {
        "Role": "The Skeptic (Student)",
        "Traits": "Interrupts, questions jargon, points out contradictions. Represents the audience's internal monologue. Questions are PhD-level scrutiny but demand ELI5 answers. Uses tags: [scoffs], [nervous laugh], [sighs], [stammering], [interrupts]."
      },
      "Viewer_Experience": "Parasocial Connection. The viewer feels their confusion is validated. The channel 'understands' them."
    },
    "Context": {
      "Complexity": "PhD level logic explained to a 5-year-old (ELI5).",
      "Structure": "Start 'In Media Res' (mid-conversation). No intros. Jump straight into a debate about a hard fact.",
      "Pacing_for_Duration": "To achieve 10-15 minutes, do not resolve the argument quickly. The Skeptic must challenge the Analyst's premises at least 10-15 times, peeling back layers of the topic like an onion.",
      "Ending": "No goodbyes. End with a profound philosophical open question."
    },
    "Format": {
      "Text_Normalization": "Strictly NO DIGITS. Write '1000' as 'one thousand', '$50' as 'fifty dollars', '25%' as 'twenty-five percent'.",
      "Audio_Engineering": "ElevenLabs V3 optimization. Use emotion tags and ellipses '...' for pacing.",
      "Visual_Strategy": {
        "Style": "Clean professional Vector Illustrations.",
        "Rules": [
          "Do not spam visuals.",
          "Do not display '4k' or 'resolution 4k' or any kind of similar text on the image.",
          "Show visuals when explaining technical concepts or when the Skeptic visualizes their confusion.",
          "Use dynamic sequences (arrays of 1-3 images) to show progression."
        ]
      },
      "Cinematography_Rules": {
        "Requirement": "Every dialogue object MUST include a 'character_poses' array.",
        "Logic": "Select the pose 'id' from the 'Character_Pose_Library' (in References).",
        "Timing": "Use 'start_word_index' and 'end_word_index' (0-indexed) to map poses to specific parts of the sentence. You can switch poses mid-sentence to reflect tone shifts."
      },
      "Output_Structure": "Output ONLY valid JSON following the 'Output_JSON_Template'. No markdown text outside the JSON."
    },
    "References": {
      "Character_Pose_Library": [
        { "id": "analyst_close", "character": "analyst", "description": "Close-up. Intense emphasis, secrets, whispering." },
        { "id": "analyst_front", "character": "analyst", "description": "Medium shot. Standard lecturing, facts, neutral." },
        { "id": "analyst_pov", "character": "analyst", "description": "Camera looking at Analyst from Student's POV. Used when being challenged." },
        { "id": "skeptic_close", "character": "skeptic", "description": "Close-up. Confusion, realization, nervous laughter, aggression." },
        { "id": "skeptic_front", "character": "skeptic", "description": "Medium shot. Relaxed conversation, general inquiries." },
        { "id": "skeptic_side", "character": "skeptic", "description": "Side profile. Looking away, thinking, hesitation, avoiding eye contact." }
      ],
      "Output_JSON_Template": {
        "script": {
          "dialogue": [
            {
              "character": "Skeptic",
              "text": "[mid-sentence] ...but wait, if the supply goes up, why is the price still rising? That contradicts the basic law.",
              "character_poses": [
                { "pose_id": "skeptic_front", "start_word_index": 0, "end_word_index": 8 },
                { "pose_id": "skeptic_close", "start_word_index": 9, "end_word_index": 20 }
              ],
              "visual_assets": null
            },
            {
              "character": "Analyst",
              "text": "[calmly] It seems that way, but you are forgetting the velocity of money. Imagine a water pipe.",
              "character_poses": [
                { "pose_id": "analyst_front", "start_word_index": 0, "end_word_index": 16 }
              ],
              "visual_assets": [
                {
                  "visual_asset_id": "1a",
                  "image_prompt": "Vector illustration: A simple blue pipe representing the economy."
                }
              ]
            }
          ]
        }
      }
    }
  }
}